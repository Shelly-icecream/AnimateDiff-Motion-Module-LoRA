ğŸ¯ ä¸€ã€ç¬¬ä¸€å¤§é—®é¢˜ï¼šæ²¡æœ‰å…ˆè·‘é€šå®˜æ–¹
âŒ ä¸€å¼€å§‹çš„é—®é¢˜
ä½ ç›´æ¥ï¼šæ”¹ç»“æ„ã€æ”¹æ•°æ®ã€æ”¹ LoRAã€æ”¹è®­ç»ƒé€»è¾‘
ä½†ï¼šå®˜æ–¹ baseline éƒ½æ²¡å®Œæ•´è·‘é€šè¿™æ˜¯æœ€å¤§é£é™©ã€‚

âœ… æ­£ç¡®æµç¨‹åº”è¯¥æ˜¯
1ï¸âƒ£ è·‘é€šå®˜æ–¹ motion finetune
2ï¸âƒ£ ç¡®è®¤æ¨¡å‹ç»“æ„
3ï¸âƒ£ æ‰“å°æ¨¡å—å
4ï¸âƒ£ éªŒè¯æ•°æ® shape
5ï¸âƒ£ å†æ³¨å…¥ LoRA

æ°¸è¿œè®°ä½ï¼š

å…ˆä¿è¯â€œå®˜æ–¹å¯è¿è¡Œâ€ï¼Œå†åšç»“æ„æ”¹é€ ã€‚

ğŸ¯ äºŒã€LoRA æ³¨å…¥é”™è¯¯é£é™©
ä½ å·®ç‚¹çŠ¯çš„å‡ ä¸ªè‡´å‘½ç‚¹
âŒ 1. æ³¨å…¥æ•´ä¸ª UNet

æœ€æ—©ä½ å†™ï¼š

if "motion" in name:

ä½†æ²¡æœ‰ç²¾ç¡®ç­›é€‰ attentionã€‚

è¿™å¯èƒ½å¯¼è‡´ï¼š

proj_in
proj_out
ff
norm
å…¨éƒ¨è¢«æ³¨å…¥
âŒ 2. åŒæ—¶è§£å†» motion module

ä½ ä¸€å¼€å§‹ï¼š

trainable_modules: ["motion_modules."]

è¿™ä¼šå¯¼è‡´ï¼š

åŸå§‹æƒé‡ + LoRA ä¸€èµ·è®­ç»ƒ

ç»“æœï¼š

ğŸ’€ å˜æˆ full finetune
âŒ 3. LoRAConv ä¸å¿…è¦

Motion Module é‡ŒçœŸæ­£é‡è¦çš„æ˜¯ï¼š

Linear attention å±‚

Conv æ³¨å…¥æ˜¯å™ªéŸ³ã€‚

âœ… ç°åœ¨çš„æ­£ç¡®åšæ³•

âœ” åªæ³¨å…¥ï¼š

motion_modules.*

âœ” ä¸”åªé’ˆå¯¹ï¼š

to_q
to_k
to_v
to_out

âœ” å†»ç»“åŸå§‹æ¨¡å—
âœ” optimizer åªæ›´æ–° lora_ å‚æ•°

ğŸ¯ ä¸‰ã€æ¨¡å—åç§°éªŒè¯é—®é¢˜

ä½ åšå¯¹çš„ä¸€ç‚¹ï¼š

print(unet.named_modules())

è¿™æ˜¯éå¸¸å…³é”®çš„ä¸€æ­¥ã€‚

å¾ˆå¤šäºº LoRA æ³¨å…¥æ ¹æœ¬æ²¡æˆåŠŸã€‚

æ­£ç¡®åšæ³•æ€»ç»“

è®­ç»ƒå‰å¿…é¡»éªŒè¯ï¼š

for name, module in unet.named_modules():
    if "motion" in name:
        print(name)

ç¡®è®¤ï¼š

motion_modules ç¡®å®å­˜åœ¨

å†å†™ç­›é€‰å‡½æ•°ï¼š

def is_motion_lora_target(name, module):
ğŸ¯ å››ã€Shape é—®é¢˜ï¼ˆæœ€å®¹æ˜“ç™½è·‘ï¼‰

AnimateDiff çš„æ­£ç¡®æ•°æ®æµæ˜¯ï¼š

Dataset:      [B, F, C, H, W]
VAE encode:   [B, C, F, H, W]
UNet è¾“å…¥:    [B, C, F, H, W]

å¦‚æœä½ ï¼š

[B,C,F,H,W]

æˆ–è€…ï¼š

[B,C,H,W,F]

ç›´æ¥ï¼š

è®­ç»ƒç™½è·‘
ğŸ¯ äº”ã€Diffusers æ¨¡å‹ vs A1111 ckpt

ä½ è¸©åˆ°è¿™ä¸ªé—®é¢˜ï¼š

ç¼º diffusion_pytorch_model.bin

åŸå› ï¼š

ckpt â‰  diffusers æ ¼å¼

AnimateDiff è®­ç»ƒå¿…é¡»æ˜¯ï¼š

stable-diffusion-v1-5/
  unet/
  vae/
  text_encoder/

ä¸æ˜¯ï¼š

model.ckpt
ğŸ¯ å…­ã€DDP å¯åŠ¨é”™è¯¯

ä½ é‡åˆ°ï¼š

KeyError: 'RANK'

å› ä¸ºï¼š

python train.py

ä¸æ˜¯ï¼š

torchrun --nproc_per_node=1 train.py

è¿™æ˜¯ PyTorch åˆ†å¸ƒå¼æœºåˆ¶è¦æ±‚ã€‚

ğŸ¯ ä¸ƒã€optimizer é™·é˜±

å¦‚æœå†™ï¼š

optimizer = AdamW(unet.parameters())

å“ªæ€• requires_grad=Falseï¼š

optimizer ä»ç„¶ä¼šéå†å…¨éƒ¨å‚æ•°

è™½ç„¶ä¸ä¼šæ›´æ–°ï¼Œä½†ï¼š

æ€§èƒ½ä¸‹é™
æ˜¾å­˜é¢å¤–å¼€é”€

æ­£ç¡®åšæ³•ï¼š

trainable_params = filter(lambda p: p.requires_grad, unet.parameters())
ğŸ¯ å…«ã€è¶…å‚æ•°é—®é¢˜

LoRA æ¨èï¼š

å‚æ•°	å»ºè®®
rank	4 æˆ– 8
alpha	= rank
lr	1e-4
weight_decay	0
batch	å°ä¸€ç‚¹
gradient_checkpoint	å¼€
ğŸ¯ ä¹ã€æ ‡ç­¾è®¾è®¡é—®é¢˜ï¼ˆä½ é—®çš„â€œé£æ ¼å¼€å¤´â€ï¼‰

Motion LoRA è®­ç»ƒæ—¶ï¼š

Prompt ç»“æ„å»ºè®®ï¼š

style_tag, subject, action, environment

ä¾‹å¦‚ï¼š

slow motion, cinematic, girl running, sunset beach

ä¸ºä»€ä¹ˆé£æ ¼æ”¾å‰é¢ï¼Ÿ

CLIP ç¼–ç å™¨å¯¹å‰éƒ¨ token æƒé‡æ›´é«˜ã€‚

ğŸ¯ åã€æœ€å¤§ç»“æ„è®¤çŸ¥è¯¯åŒº

å®é™…ç»“æ„æ˜¯ï¼š

down_blocks.X.motion_modules.Y.temporal_transformer.transformer_blocks.0.attention_blocks.Z

Z æ‰æ˜¯ï¼š

0 = self attention
1 = cross attention

Self Attention
frame1 â†â†’ frame2 â†â†’ frame3 â†â†’ frame4

æ—¶é—´å†…éƒ¨äº’ç›¸å½±å“ã€‚

Cross Attention
frame features  â†â†’  text embedding

è§†é¢‘å»çœ‹æ–‡æœ¬ã€‚

ğŸ§  ä½ è¿™æ¬¡è®­ç»ƒçœŸæ­£çš„æ ¸å¿ƒè®¤çŸ¥

ä½ ç°åœ¨åœ¨åšçš„ä¸æ˜¯ï¼š

â€œå¾®è°ƒä¸€ä¸ªæ¨¡å‹â€

ä½ åœ¨åšçš„æ˜¯ï¼š

åœ¨ temporal transformer é‡Œæ’å…¥ä½ç§©é€‚é…å™¨æ¥æ”¹å˜æ—¶é—´åŠ¨æ€å»ºæ¨¡

è¿™æ˜¯ç»“æ„çº§æ”¹é€ ï¼Œä¸æ˜¯å‚æ•°çº§å¾®è°ƒã€‚