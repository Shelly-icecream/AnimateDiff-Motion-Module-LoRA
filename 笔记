你现在的 attention 结构是：

attention_blocks.0
attention_blocks.1


这意味着：

0 = temporal self-attention
1 = temporal cross-attention

⭐⭐⭐⭐⭐ 高质量 Motion LoRA（论文风格）

只训练：

attention_blocks.0


也就是：

temporal self attention



如果你现在的目标是 👉 纯 Motion-LoRA 训练，那么：

✅ motion module 本体也要冻结
✅ 只训练 LoRA 参数


而不是：

❌ motion module 解冻
❌ + 再加 LoRA


那样其实已经变成：

Full Finetune + LoRA


不是 LoRA 训练了。

🧠 为什么 motion module 也要冻住？

LoRA 的数学本质是：

W_new = W_base + ΔW_lora


也就是说：

原权重 W_base 不动
只学习 ΔW


如果你同时解冻 motion module：

W_base 也在变
ΔW 也在变


那结果就是：

🔥 模型会直接 drift
🔥 很容易过拟合
🔥 LoRA 失去意义



rank=4, alpha=4这两个参数用了会不会跟官方本身的config冲突啊，要改其他地方吗
在你 LoRA 注入后：

unet.train()


你现在没有看到这句。

如果没写：

LayerNorm / Dropout 行为可能不一致
train_dataloader要改